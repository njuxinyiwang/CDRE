[training]
batch_size = 8
gradient_accumulation_steps = 1
total_round = 6
drop_out = 0.5
num_workers = 2
step1_epochs = 30
step2_epochs = 30
device = cuda
seed = 100
max_grad_norm = 10
task_length = 5
kl_temp = 2
f_pass = 10
margin = 0.15
eps=1e-8



[Encoder]
bert_path = ./bert-base-uncased
transformer_type = bert
max_length = 256
vocab_size = 30522
marker_size = 4
pattern = entity_marker
distance_metric = dot_product
encoder_output_size = 768
hidden_size = 768
gamma = 0.6
nota_rectification_factor = 0.1